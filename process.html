<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speak Your Mind</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js" integrity="sha384-7+zCNj/IqJ95wo16oMtfsKbZ9ccEh31eOz1HGyDuCQ6wgnyJNSYdrPa03rtR1zdB" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js" integrity="sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13" crossorigin="anonymous"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;700&display=swap" rel="stylesheet"> 
    
    <link rel="stylesheet" href="./style/style.css" />
</head>
<body>
    <div class="container">
       <!-- header -->
        <nav class="row d-flex justify-content-between mt3">
          <div class="col-8">
            <h1>Speak Your Mind</h1>
          </div>
          <div class="col-4">
              <div class="row">
            <div class="col">
                <a href="process.html">Process</a>
            </div>
            <div class="col">
                <a href="resources.html">Resources</a>
            </div>
            <div class="col">
                <a href="contact.html">Contact Us</a>
            </div>
        </div>
          </div>
        </nav>
        <!-- section 1: Our process -->
        <div class="container mt5 main">
            <div class="row">
                <h1>Our process</h1>
            </div>
            <div class="row content">
                <div class="col">
                    <p class="type-style-1">Having had the opportunity to engage with children from Rainbow Centre Singapore as artists-in-residence in the Peekaboo! Inclusive Arts Festival by SuperHero Me in 2018, both Fong Yee and myself witnessed first-hand how various assistive technological devices that were being used by the children in school could be incredibly limiting and painfully non-intuitive, especially when it comes to self-expression. With technology being omnipresent in our everyday lives, it was hard for us to imagine that better, more creative alternatives for these children to express themselves seemed lacking. use of existing technology to communicate their thoughts and needs/wants. 
                    </p>
                    <!-- add image -->
                    <p>The experience planted the seed within us that we could do more to enable self-expression through alternative means. We believe in the use of technology as an enabler of self-expression and meaningful interactions. Whilst numerous technologically innovative tools exist within the machine learning and programming community, they often feel ‘out of reach’ and seem ‘too complex’ for anyone to ‘just install and make use of’. There seems to be an implicit assumption by developers of the tools that the people who are and will be using them have a similar level of knowledge and understanding in how to use them, which is most definitely untrue. This inspired us to build tools from a ground up level that are enabling via its simplicity rather than disabling through its complexity.
                    </p>
                    
                    <p>We had hoped to run hands-on experience sessions for participants to experiment with the prototypes during the developmental stage of the project, and have our work process be guided by the childrens’ interactions and responses with the tools. Due to the extended Covid-19 situation over the course of the project timeline, we were unable to actualise this. That said, we are extremely grateful to have been able to converse with numerous people who interact daily with children with special needs, all of whom have been very forthcoming and open in sharing about their experiences as well as giving us feedback on our work-in-progress. 
                    </p>

                    <p>We wholeheartedly wish to thank the following persons: 
                    </p>

                    <p>Mdm Hidayah and Riqz, Ms Joyce Teo, Ms Michelle Cheong, Ms Elizabeth Quek, Mr Tan Chuan Hoh, Ms Saxena Chandhok Tanushree, Ms Sarah Yong, Mr Tan Yeok Nguan, Ms Evelyn Ang, Mr Alvin Tan, Ms Elaine Ng, and Ms Chen Weiyan
                    </p>

                    <p>One of the key take-aways from every conversation is the lack of peer-to-peer interaction and communication in the daily lives of the children with special needs, especially if they are non-verbal. Other important considerations include: bulkiness of devices, the difficulty and cost of integrating it into daily life activities, language ability, any motor skill ability which could affect the way each individual is able (or not) to make use of existing technology to communicate their thoughts and needs/wants. 
                    </p>

                    <!-- add image -->
                    <p>In publishing this website, we hope that these tools can become a starting point for more people to ‘say’ something, to express themselves in a way that gives them a ‘voice’.  
                    </p>
                </div>
            </div>
        </div>
        <!-- section 2: Our work process and early experiences -->
        <div class="container mt5 main">
            <div class="row">
                <h1>Our work process and early experiences</h1>
            </div>
            <div class="row content">
                <div class="col">
                    <p class="type-style-1">We were very much inspired by Experiments with Google projects such as Look to Speak, TensorFlow Lite for Microcontrollers, and Body Synth amongst others. This led to early experiments with soft switches made from conductive yarn, microcontrollers and using external devices such as the LeapMotion together with Touch Designer to detect gesture and movement, translating that into moving an object on screen, or making a drawing. 
                    </p>
                    <!-- add image -->
                    <p>After we had the chance to speak with educators, caregivers and others working with people with special needs, we realised that it made sense to focus on using technology available on existing consumer devices (such as computers with built-in webcams). The decision to avoid using new hardware to create our tools was so that it would reduce the barrier to entry for anyone to try out the tools. Think less bulk, intuitive features, and a tool that does not require cumbersome switches or add-on devices. As hand gesture and/or movement could easily be tracked using a webcam, it made sense to use a hand gesture driven interface for our drawing tool. 
                    </p>
                    <p>Since the range of mobility that each child has is unique, we began to include a gesture calibration option in the early stages of our prototype testing. Not limiting it to a fixed gesture (i.e an open palm and closed fist) allowed for , and the child can choose whichever 2 gestures they are comfortable with to create their drawing.
                    </p>
                    
                    <!-- add image -->
                    <p>We imagine that there can be many applications of this hand gesture interface, making a drawing is merely the beginning of a mode of communication/expression. With different outputs, this could be an alternative to a touchscreen cursor (many children with limited hand mobility also have difficulties using a touchscreen), a switch controller, or a custom physiotherapy experience.
                    </p>
                </div>
            </div>
        </div>
        <!-- section 3: Some header -->
        <div class="container mt5 main">
            <div class="row">
                <h1>Some header</h1>
            </div>
            <div class="row content">
                <div class="col">
                    <p class="type-style-1">We imagine that there can be many applications of this hand gesture interface, making a drawing is merely the beginning of a mode of communication/expression. With different outputs, this could be an alternative to a touchscreen cursor (many children with limited hand mobility also have difficulties using a touchscreen), a switch controller, or a custom physiotherapy experience.
                    </p>
                        <!-- Point form -->
                    1. It takes a long time for the children to find each word (or they may not be able to find the correct word) they wanted to say using their physical PODD books or their digital AAC apps, and being able to finish a sentence / share an idea is important before the conversation moves on to something else.
                    2. Caregivers occasionally were unable to guess what the children wanted to say, resulting in the children feeling frustrated and giving up on communicating that thought.
                    <!-- Quote -->
                    “To say something like “I want to watch TV.” will require several pages of flipping and going through 1 word at a time.” 
                    <!-- Quote caption -->
                    —Mdm Hidayah on how long it would take Riqz to make a statement occasionally
                    <!-- cont para -->
                    Sometimes the caregiver or teacher might resort to using Google search and/or Google image search to narrow the word options. That made us wonder, would it be possible to find a way to enable the children to communicate more intuitively? 

                    Could something as ubiquitous as auto-complete and auto-prediction be implemented in AAC apps? Wouldn’t it be great to have this option accessible and available to non-verbal children using digital AAC apps since it is such a natural way of communicating on a screen?

                    After speaking to Donn Koh from STUCK Design, he proposed we pare down the functions of the typical AAC app and focus on the auto-prediction using machine learning. We will simplify the interface and have a few of the most commonly used words to appear right at the first few pages for the person to be able to find them quickly. 
                    <!-- add image -->
                    We do not imagine that the AAC add-on prototype will replace any current system of AAC that is being used at the moment. Unlike an AAC app which is in itself a holistic language system, our app add-on is really a supporting tool, trying to complement what the AAC does. Similar to the relationship between an Apple watch and Iphone. The Apple watch acts as a companion device to the phone, allowing quick access to make a call or receive notifications and messages whilst the Iphone would have all the functions needed. Our prototype app is meant for use on-the-go, showing you words you would use most often to communicate something quickly to someone. 

                    Our goal is to see if auto-prediction (together with a pared down user interface) makes a difference in being able to say something more quickly.  

                </div>
            </div>
        </div>
      </div>
</body>
</html>