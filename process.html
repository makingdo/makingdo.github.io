<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>speak your mind</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3"
      crossorigin="anonymous"
    />
    <script
      src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js"
      integrity="sha384-7+zCNj/IqJ95wo16oMtfsKbZ9ccEh31eOz1HGyDuCQ6wgnyJNSYdrPa03rtR1zdB"
      crossorigin="anonymous"
    ></script>
    <script
      src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js"
      integrity="sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13"
      crossorigin="anonymous"
    ></script>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;700&display=swap"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./style/style.css" />
  </head>
  <body>
    <div class="container">
      <!-- header -->
      <nav class="row d-flex justify-content-between mt3">
        <div class="col-8">
          <a href="index.html">
            <h1>speak your mind</h1>
          </a>
        </div>
        <div class="col-4">
          <div class="row">
            <div class="col">
              <a href="process.html">Process</a>
            </div>
            <div class="col">
              <a href="resources.html">Resources</a>
            </div>
            <div class="col">
              <a href="contact.html">Contact Us</a>
            </div>
          </div>
        </div>
      </nav>
      <!-- section 1: How it started -->
      <div class="container mt4 main">
        <div class="row">
          <h1>How it started</h1>
        </div>
        <div class="row content">
          <div class="col">
            <p class="">
              Having had the opportunity to engage with children from <strong>Rainbow
              Centre Singapore</strong> as artists-in-residence in the <span><a class="visible-link" href="https://www.superherome.sg/peekaboo" target="_blank">Peekaboo!
              Inclusive Arts Festival</a> by <strong>SuperHero Me</strong> in 2018, both Fong Yee and
              myself witnessed first-hand how various assistive technological
              devices that were being used by the children in school could be
              incredibly limiting and painfully non-intuitive, especially when
              it comes to self-expression. With technology being omnipresent in
              our everyday lives, it was hard for us to imagine that better,
              more creative alternatives for these children to express
              themselves were not available to them.
            </p>
            <img class="image mb3" src="./assets/SHM_Liping.png">
            <p>
              The experience prompted us to look into tech to
              enable self-expression through alternative means. We believe in
              the use of technology as an enabler of self-expression and
              meaningful interactions. Whilst numerous technologically
              innovative tools exist within the machine learning and programming
              community, they often feel ‘out of reach’ and seem ‘too complex’
              for anyone to ‘install and make use of’. There seems to be an
              implicit assumption by developers of the tools that the people who
              are and will be using them have a similar level of knowledge and
              understanding in how to use them, which is not always the case.
              This inspired us to build tools from a ground up level that are
              enabling via its simplicity rather than disabling through its
              complexity.
            </p>

            <p>
              We had hoped to run hands-on experience sessions for participants
              to experiment with the prototypes during the developmental stage
              of the project, and have our work process be guided by the
              childrens’ interactions and responses with the tools. Due to the
              Covid-19 situation over the course of the project's timeline,
              we were unable to actualise this. That said, we are
              extremely grateful to have been able to converse with numerous
              people who interact daily with children with special needs, all of
              whom have been very forthcoming and open in sharing about their
              experiences as well as giving us feedback on our work-in-progress.
            </p>

            <p>We wholeheartedly wish to thank the following persons:</p>

            <p>
              Mdm Hidayah and Rizq, Ms Joyce Teo, Ms Michelle Cheong, Ms
              Elizabeth Quek, Mr Tan Chuan Hoh, Ms Saxena Chandhok Tanushree, Ms
              Sarah Yong, Mr Tan Yeok Nguan, Ms Evelyn Ang, Mr Alvin Tan, Ms
              Elaine Ng, and Ms Chen Weiyan
            </p>

            <p>
              One of the key take-aways from every conversation is the lack of
              peer-to-peer interaction and communication in the daily lives of
              the children with special needs, especially if they are
              non-verbal. Other important considerations include: bulkiness of
              devices, the difficulty and cost of integrating it into daily life
              activities, language ability, any motor skill ability which could
              affect the way each individual is able (or not) to make use of
              existing technology to communicate their thoughts and needs/wants.
            </p>

            <!-- add image -->
            <img class="image image-50" src="./assets/ChatWithHidayah.png">
            <p class="caption">
              Screenshot of Mdm Hidayah demonstrating to us the bulkiness of an attachment device for attaching a switch 
            <br/>
              Image credit: Ng FongYee
            </p>
            <p>
              In publishing this website, we hope that these tools can become a
              starting point for more people to ‘say’ something, to express
              themselves in a way that gives them a ‘voice’.
            </p>
          </div>
        </div>
      </div>
      <!-- section 2: Process: Gesture drawing -->
      <div class="container mt4 main">
        <div class="row">
          <h1>Process: Gesture drawing</h1>
        </div>
        <div class="row content">
          <div class="col">
            <!-- add image -->
            <img class="image" src="./assets/RemoteWorkDiscussion.png">
            <p class="caption">
              Screenshot of one of our numerous zoom meetings as we were based in Singapore and Tokyo. 
            <br/>
              Image credit: Ng FongYee
            </p>

            <!-- add image -->
            <div class="row">
              <div class="col">
                <img class="image mt0" src="./assets/Ideas 1.jpeg">
              </div>
              <div class="col">
                <img class="image mt0" src="./assets/Ideas 2.jpeg">
              </div>
              <p class="caption">
                Discussion points on needs, senses, body movements and types of technology to consider 
                <br/>
                Image credit: Chan Li Ping
              </p>
            </div>
            <p class="">
              We were very much inspired by <strong>Experiments with Google</strong> projects
              such as <a class="visible-link" href="https://experiments.withgoogle.com/looktospeak" target="_blank">Look to Speak</a>, 
              <a class="visible-link" href="https://experiments.withgoogle.com/collection/tfliteformicrocontrollers" target="_blank">TensorFlow Lite for Microcontrollers</a>, and
              <a class="visible-link" href="https://experiments.withgoogle.com/body-synth" target="_blank">Body Synth</a> amongst others. This led to early experiments with soft
              switches made from conductive yarn, microcontrollers and using
              external devices such as the LeapMotion together with Touch
              Designer to detect gesture and movement, translating that into
              moving an object on screen, or making a drawing.
            </p>
            <!-- add image -->
            <div class="row">
              <div class="col">
                <img class="image mt0" src="./assets/Conductive yarn squeeze ball.jpeg">
              </div>
              <div class="col">
                <img class="image mt0" src="./assets/Conductive yarn switches and sensors test.jpeg">
              </div>
              <p class="caption">
                Experimenting with soft switches 
                <br/>
                Image credit: Chan Li Ping
              </p>
            </div>
            </div>
            <p>
              After we had the chance to speak with educators, caregivers and
              others working with people with special needs, we realised that it
              made sense to focus on using technology available on existing
              consumer devices (such as computers with built-in webcams). The
              decision to avoid using new hardware to create our tools was so
              that it would reduce the barrier to entry for anyone to try out
              the tools. Think less bulk, intuitive features, and a tool that
              does not require cumbersome switches or add-on devices. With this 
              in mind, we felt that it made sense to use webcam hand-tracking as 
              and interface for our drawing tool.
            </p>
            <p>
              Since the range of mobility that each child has is unique, we
              decided to include an option for gesture calibration in the early stages
              of our prototype testing. Not limiting it to a fixed gesture (i.e
              an open palm and closed fist) allowed for customisation, and the child can
              choose whichever 2 gestures they are comfortable with to create
              their drawing.
            </p>

            <!-- add image -->
            <img class="image" src="./assets/EarlyHandGestureCalibration.png">
            <p class="caption">
              Screenshot of early hand gesture calibration test
              <br/>
              Image credit: Ng FongYee
            </p>

            <p>
              We imagine that there can be many applications of this hand
              gesture interface, making a drawing is merely the beginning of a
              mode of communication/expression. With different outputs, this
              could be an alternative to a touchscreen cursor (many children
              with limited hand mobility also have difficulties using a
              touchscreen), a switch controller, or a custom physiotherapy
              experience etc
            </p>
          </div>
        </div>
      </div>
      <!-- section 3: Process AAC Add-on -->
      <div class="container mt4 main">
        <div class="row">
          <h1>Process: AAC Add-on</h1>
        </div>
        <div class="row content">
          <div class="col">
            <p class="">
              A hope many caregivers and educators have for non-verbal/minimally-verbal 
              children with special needs is that they will be able to use their AAC devices 
              independently to communicate. Some common reflections include:
            </p>
            <ol>
              <!-- Point form -->
              <li>
                It takes a long time for the children to find each word (or they
                may not be able to find the correct word) they wanted to say
                using their physical PODD books or their digital AAC apps, and
                being able to finish a sentence / share an idea is important
                before the conversation moves on to something else.
              </li>
              <br/>
              <li>
              Caregivers occasionally were unable to guess what the children
              wanted to say, resulting in the children feeling frustrated and
              giving up on communicating that thought.
            </li>
            </ol>
            <!-- Quote -->
            <p class="quote mt2">
            “To say something like “I want to watch TV.” will require several
            pages of flipping and going through 1 word at a time.”
            </p>
            <!-- Quote caption -->
            <p class="caption mb2">
            —Mdm Hidayah on how long it would take Rizq to make a statement
            occasionally
            </p>
            <!-- cont para -->
            <p>Sometimes the caregiver or teacher might resort to using Google
            search and/or Google image search to narrow the word options. That
            made us wonder, would it be possible to find a way to enable the
            children to communicate more intuitively?</p>
            <p>Could something as
            ubiquitous as auto-complete and auto-prediction be implemented in
            AAC apps? Wouldn’t it be great to have this option accessible and
            available to non-verbal children using digital AAC apps since it is
            such a natural way of communicating on a screen? </p>
            
            <p>After speaking to
            Donn Koh from STUCK Design, he proposed we pare down the functions
            of the typical AAC app and focus on the auto-prediction using
            machine learning. We will simplify the interface and have a few of
            the most commonly used words to appear right at the first few pages
            for the person to be able to find them quickly.
            </p>
            <!-- add image -->
            <img class="image" src="./assets/EarlyAACIdea.png">
            <p class="caption">
              Early concept (3by3) for AAC app add-on
              <br/>
              Image credit: Donn Koh
            </p>

            <p class="">
            We do not imagine that the AAC add-on prototype will replace any
            current system of AAC that is being used at the moment. Unlike an
            AAC app which is in itself a holistic language system, our app
            add-on is a supporting tool, trying to complement what the
            AAC app does. Similar to the relationship between an Apple watch and
            iphone. The Apple watch acts as a companion device to the phone,
            allowing quick access to make a call or receive notifications and
            messages whilst the iphone would have all the functions needed. Our
            prototype app is meant for use on-the-go, showing you words you
            would use most often to communicate something quickly to someone.
            </p>
            <p class="">
            Our goal is to see if auto-prediction (together with a pared down
            user interface) makes a difference in being able to say something
            more quickly.
            </p>
          </div>
        </div>
      </div>
    </div>
    <footer class="footer"></footer>
  </body>
</html>
